{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming for Data Science and Artificial Intelligence\n",
    "\n",
    "## 0. Linear Regression Extension\n",
    "\n",
    "You have learned all the detail of Linear Regression. Its core idea is presented in the lecture.\n",
    "\n",
    "To enhance the `Gradient` function, there are two extensions that improve the optimization process in terms of quality of life (your life).\n",
    "\n",
    "1. Early Stopping\n",
    "2. Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "#print the shape of X and y\n",
    "X.shape, y.shape\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1\n",
    "\n",
    "#print one row of X, and maybe try to see what it is...\n",
    "#print one row of y, and maybe try to see what it is....\n",
    "# X[0]\n",
    "# y[0]\n",
    "# diabetes.feature_names\n",
    "# label is blood glucose level.....\n",
    "\n",
    "#please help me set m and \n",
    "m = X.shape[0]  #number of samples\n",
    "n = X.shape[1]  #number of features\n",
    "\n",
    "#write an assert function to check that X and y has same amount of samples...\n",
    "assert m == y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#split here\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size = 0.3, random_state = 9999\n",
    ")\n",
    "\n",
    "#assert that X_train and y_train have the same amount of samples\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "\n",
    "#assert that X_test and y_test have the same amount of samples\n",
    "assert X_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "#standardize the training set\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "#standardize the test set\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "intercept.shape\n",
    "\n",
    "#hint: use np.concatenate with X_train on axis=1, to add these ones to X_train\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "intercept.shape\n",
    "\n",
    "#hint: use np.concatenate with X_test on axis=1, to add these ones to X_test\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put everything fit()\n",
    "\n",
    "#1. randomize our theta\n",
    "#please help me create a random theta of size (X_train.shape[1], )\n",
    "theta = np.ones(X_train.shape[1])\n",
    "#why X_train.shape[1]\n",
    "\n",
    "#5. repeat 2, 3, 4\n",
    "#please put a for loop for 2, 3, 4, for 1000 times\n",
    "#set 1000 call it max_iter\n",
    "#for _ in range(max_iter):\n",
    "max_iter = 1000\n",
    "alpha = 0.0001\n",
    "\n",
    "def predict(X, theta):\n",
    "    return X @ theta\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "def _grad(X, error):  #it's only for internal purpose\n",
    "    #not for external purpose\n",
    "    #what do you mean by external:  i mean the main() program of python\n",
    "    return X.T @ error\n",
    "\n",
    "def fit(X_train, y_train, theta, max_iter, alpha):\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        #2. predict\n",
    "        yhat = predict(X_train, theta)  #put this into a function called predict(X_train, theta)\n",
    "\n",
    "        #2.1 can you guys compute the squared error\n",
    "        # squared_error = ((yhat - y_train) ** 2).sum()\n",
    "        #print the mean squared error, we can see whether MSE goes down eventually...\n",
    "        mse =  mean_squared_error(y_train, yhat)\n",
    "        if(i % 50 == 0):\n",
    "            print(f\"MSE: {mse}\")  \n",
    "\n",
    "        #3. get derivatives\n",
    "        deriv = _grad(X_train, yhat - y_train)\n",
    "\n",
    "        #4. update weight\n",
    "        theta = theta - alpha * deriv\n",
    "        \n",
    "    return theta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 28562.951917344537\n",
      "MSE: 3897.336230339511\n",
      "MSE: 2877.3736974835792\n",
      "MSE: 2831.2558171055753\n",
      "MSE: 2827.9119296024915\n",
      "MSE: 2826.5530590194744\n",
      "MSE: 2825.334848687645\n",
      "MSE: 2824.1622448159806\n",
      "MSE: 2823.0266260126527\n",
      "MSE: 2821.925301403898\n",
      "MSE: 2820.8564421902097\n",
      "MSE: 2819.8185235878746\n",
      "MSE: 2818.810206706694\n",
      "MSE: 2817.8302933929094\n",
      "MSE: 2816.877699352769\n",
      "MSE: 2815.951434445826\n",
      "MSE: 2815.0505871961686\n",
      "MSE: 2814.1743123627202\n",
      "MSE: 2813.3218208956955\n",
      "MSE: 2812.492371795244\n"
     ]
    }
   ],
   "source": [
    "theta = fit(X_train, y_train, theta, max_iter, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3079.2424482139854"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = predict(X_test, theta)\n",
    "\n",
    "mean_squared_error(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Code from the class\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "def mean_squared_error(ytrue, ypred):\n",
    "    return ((ypred - ytrue) ** 2).sum() / ytrue.shape[0]\n",
    "\n",
    "class LinearRegression(object):\n",
    "    \n",
    "    kfold = KFold(n_splits=5)\n",
    "            \n",
    "    def __init__(self, alpha=0.001,early_stopping = True,max_iters = 1000,loss_old=10000, tol=1e-3,   \n",
    "            method='batch', batch_size=100):\n",
    "        self.alpha      = alpha\n",
    "        self.early_stopping = early_stopping\n",
    "        self.batch_size = batch_size\n",
    "        self.method     = method\n",
    "        self.loss_old = 10000\n",
    "        self.tol = tol\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        \n",
    "        #using training......\n",
    "        \n",
    "        #please change it to cross-validation.....\n",
    "        \n",
    "        #create a list of kfold scores\n",
    "        self.kfold = list()\n",
    "\n",
    "        #Kfold.split in the sklearn.....\n",
    "        #5 splits\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv.split(X_train)):\n",
    "            \n",
    "            X_cross_train = X_train[train_idx]\n",
    "            y_cross_train = y_train[train_idx]\n",
    "            X_cross_val   = X_train[val_idx]\n",
    "            y_cross_val   = y_train[val_idx]\n",
    "            \n",
    "            #create self.theta here\n",
    "            self.theta = np.zeros(X_cross_train.shape[1])\n",
    "            \n",
    "            #define X_cross_train as only a subset of the data\n",
    "            #how big is this subset?  => mini-batch size ==> 50\n",
    "            \n",
    "            #one epoch will exhaust the WHOLE training set\n",
    "            for epoch in range(self.num_epochs):\n",
    "            \n",
    "                #with replacement or no replacement\n",
    "                #with replacement means just randomize\n",
    "                #with no replacement means 0:50, 51:100, 101:150, ......300:323\n",
    "                #shuffle your index\n",
    "                #===> please shuffle your index\n",
    "                perm = np.random.permutation(X_cross_train.shape[0])\n",
    "                        \n",
    "                X_cross_train = X_cross_train[perm]\n",
    "                y_cross_train = y_cross_train[perm]\n",
    "                \n",
    "                if   self.method == 'sto':\n",
    "                    for batch_idx in range(X_cross_train.shape[0]):\n",
    "                        X_method_train = X_cross_train[batch_idx].reshape(1, -1) #(11,) ==> (1, 11) ==> (m, n)\n",
    "                        y_method_train = y_cross_train[batch_idx]                    \n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                elif self.method == 'mini':\n",
    "                    for batch_idx in range(0, X_cross_train.shape[0], self.batch_size):\n",
    "                        #batch_idx = 0, 50, 100, 150\n",
    "                        X_method_train = X_cross_train[batch_idx:batch_idx+self.batch_size, :]\n",
    "                        y_method_train = y_cross_train[batch_idx:batch_idx+self.batch_size]\n",
    "                        self._train(X_method_train, y_method_train)\n",
    "                else:\n",
    "                    X_method_train = X_cross_train\n",
    "                    y_method_train = y_cross_train\n",
    "                    self._train(X_method_train, y_method_train)\n",
    "                    \n",
    "            yhat_val = self.predict(X_cross_val)\n",
    "            self.kfold.append(mean_squared_error(y_cross_val, yhat_val))\n",
    "            print(f\"Fold {fold}: {mean_squared_error(y_cross_val, yhat_val)}\")\n",
    "                    \n",
    "    def _train(self, X, y):\n",
    "        yhat = self.predict(X)\n",
    "        grad = X.T @(yhat - y)\n",
    "        self.theta = self.theta - self.alpha * grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X @ self.theta  #===>(m, n) @ (n, )\n",
    "    \n",
    "    def _coef(self):\n",
    "        return self.theta[1:]  #remind that theta is (w0, w1, w2, w3, w4.....wn)\n",
    "                               #w0 is the bias or the intercep\n",
    "                               #w1....wn are the weights / coefficients / theta\n",
    "        \n",
    "    def _bias(self):\n",
    "        return self.theta[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Early Stopping\n",
    "\n",
    "`num_epochs` is the parameter that specify how many iteration your model will perform the optimization.\n",
    "\n",
    "The question to ask is \"How many `num_epochs` is enough?\".\n",
    "\n",
    "The answer is when your model is optimized.\n",
    "\n",
    "Optimized? What is optimized?\n",
    "\n",
    "Ah.. young padawan, remember this figure?\n",
    "\n",
    "<img width=\"400\" src = \"figures/gradient.png\">\n",
    "\n",
    "We say the model is trained/optimized/learned when *it* reachs the minima, preferably, global minima.\n",
    "\n",
    "Our goal of optimization in this Gradient context is to minimize the loss/error.\n",
    "\n",
    "$$ \\min_{\\theta} || \\hat{y} - y ||^2_2 $$\n",
    "\n",
    "$\\theta$ is vector of weights to optimize.\n",
    "\n",
    "$\\hat{y}$ is actually a result of $ \\mathbf{X}\\theta $.\n",
    "\n",
    "$y$ is a vector of ground-truth/label.\n",
    "\n",
    "$|| . ||^2_2$ is the loss/error function. \n",
    "\n",
    "Now, you remember your objective. It is to minimize the $|| . ||^2_2$ by adjusting the $ \\theta $\n",
    "\n",
    "Then, the model is done training when you finally get the $ \\theta $ that yeild the best result (lowest error / minima).\n",
    "\n",
    "### How do you know you are there at the minima?\n",
    "\n",
    "Well, one way is to tracking your loss over the training session. \n",
    "\n",
    "![loss](https://miro.medium.com/max/411/1*UHmMMH3OhrBvgw18jKiy6Q.png)\n",
    "\n",
    "You see, when you plot the loss/error (in the figure is cost) over the epochs, it should be desending and finally plateau.\n",
    "\n",
    "When it is plateau, in other words, adjusting $\\theta$ no longer improve the prediction. You are at the minima. (Gradient is 0 at minima).\n",
    "\n",
    "According to the plot of lost/error, when do we reach the minima? 3000?, 2000?, 1000?\n",
    "\n",
    "In this case, do you need to set `num_epochs` to 3000?\n",
    "\n",
    "In the next dataset you will work on, what `num_epochs` you need to set?\n",
    "\n",
    "Ah.. You will never know until you train the model with that new dataset, at least, one time.\n",
    "\n",
    "Can we do better?\n",
    "\n",
    "Yes, we can. Introducing!!!!!!!!! `Early Stopping`!!!!!!!!!\n",
    "\n",
    "### Early Stopping\n",
    "\n",
    "The idea of `early stoping` is, as the name suggest, break the training before it reachs the `num_epochs`.\n",
    "\n",
    "The key is **no longer improve**. **Improve!!**. What is improve?\n",
    "\n",
    "Improve is the loss of iteration/epoch *i* is lower than the *i-1*.\n",
    "\n",
    "How much is the $\\text{loss}_i - \\text{loss}_{i-1}$ so that we say it is improving? This is up to you. \n",
    "\n",
    "Now code!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (2309294821.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "### pesudo code for people who has no idea\n",
    "\n",
    "# for _ in range(epochs)\n",
    "#   train(x_train, y_train)\n",
    "#   loss = error(predict(x_train) - y_train)\n",
    "#   if prev_loss - loss > theshold\n",
    "#      ok, there is an improment\n",
    "#      prev_loss = loss\n",
    "#   else\n",
    "#      done training. break\n",
    "\n",
    "if self.early_stopping == True: #for early stopping\n",
    "                yhat = self.predict(X)\n",
    "                new_loss = self.mse(yhat,y)\n",
    "                if np.abs(self.loss_old - new_loss) < self.tol:\n",
    "                    print('iteration stopped at = ' + str(j))\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Momentum\n",
    "\n",
    "`alpha` or `learning_rate` is the parameter how big of the step you will take on each Gradient optimization step.\n",
    "\n",
    "![overshoot](https://miro.medium.com/max/700/1*hGhRddOUV8h0pdQek8T35A.png)\n",
    "\n",
    "How much is `alpha` you should set?\n",
    "\n",
    "Not big such that it causes the *over shooting*.\n",
    "\n",
    "And certainly, not too small such that it causes you extra epochs to reach the minima.\n",
    "\n",
    "You want **just right** `alpha`.\n",
    "\n",
    "How do you know what is the right `alpha`?\n",
    "\n",
    "You guess, right?\n",
    "\n",
    "This time, 0.1. Observe the loss. If it is overshooting, stop and reduce the `alpha`. If the loss reduce slightly, maybe stop and increase the `alpha`.\n",
    "\n",
    "Finally, if you spend time adjusting the `alpha`, you will eventually find the **just right** `alpha`.\n",
    "\n",
    "Can we do better? Can we reduce the number of epochs without searching for the **just right** `alpha`.\n",
    "\n",
    "Yes, we can!!! Introducing!!!!!!!! MOMENTUM~~!!!!!!!!!\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Here is the updating equation.\n",
    "\n",
    "$$ \\theta^{i+1} = \\theta^{i} - \\alpha \\times \\text{gradient}^i $$\n",
    "\n",
    "$i$ is the iteration/epoch number.\n",
    "\n",
    "$\\theta^{i+1}$ is the updated weights and will be used for the next iteration.\n",
    "\n",
    "$\\theta^{i}$ is the current weights of iteration $i$.\n",
    "\n",
    "$\\alpha$ is the learning rate.\n",
    "\n",
    "$\\text{gradient}^i$ is the step suggesting by the deviation of loss function from iteration $i$.\n",
    "\n",
    "$\\alpha \\times \\text{gradient}^i$ can be called *step*. When consider $i$, we call the $step^i$ how big of the step you take in iteration $i$.\n",
    "\n",
    "We know, in the early iteration, the step will be very big. Given the `alpha` is not too big, the step will decrase over time.\n",
    "\n",
    "Let's say our step size was big in the previous iteration (i-1), maybe, instead of taking $step^i$, we can take bigger step than $step^i$.\n",
    "\n",
    "Yes, and here is the equation.\n",
    "\n",
    "$$ \\theta^{i+1} = \\theta^{i} - step^i + \\text{momentum} \\times step^{i-1} $$\n",
    "\n",
    "Then, *momentum* here is how much do you want to include the information of previous step. The range of *momentum* is $[0.0 - 1.0]$. When *momentum* is 0 then you do not use previous step (basically, normal optimization). When it is 1 then you fully use the information from previous step. \n",
    "\n",
    "Now, code !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pesudo code for people who has no idea\n",
    "\n",
    "# step = alpha * grad\n",
    "# theta = theta - step + momentum * prev_step\n",
    "# theta - alpha * grad + momentum must be 1 * prev_step \n",
    "# prev_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a59b473075a889197cef78f691a8dde253fc9cd06ebdea22432c59d124001e4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
